---
title: 'TPOT: Where do I start?'
author: Trang Le
date: '2019-11-05'
slug: tpot-where-do-i-start
categories:
  - research
tags:
  - machine learning
  - informatics
  - autoML
description:
  What is automated machine learning? What do I need to know to start? While TPOT already has greatly extensive documentation and actively maintained, I figured a list of common tips and tricks may be helpful to new users.
twitterImg: 2019/11/05/tpot-where-do-i-start/tpot-slide.png
---

<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<p>Tree-based Pipeline Optimization Tool (TPOT) is an automated machine learning tool that helps the data scientist find the optimal model pipeline for their prediction problem. Using genetic programming (GP), TPOT explores different pipelines (sequences of feature selectors, model classifiers, <em>etc.</em>) and recommends one with optimal cross-validated score after a specified number of generations.</p>
<p>Here are some TPOT-related papers{{%sn%}}Some others: <a href="https://dl.acm.org/citation.cfm?id=2908918">Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science</a>, <a href="https://dl.acm.org/citation.cfm?id=3071178.3071212">Toward the automated analysis of complex diseases in genome-wide association studies using GP</a>{{%/sn%}} that will give you a quick glance at TPOT. Please let me know if you find other useful ones I should add here.</p>
<ul>
<li>The orginal TPOT paper: <a href="https://link.springer.com/chapter/10.1007%2F978-3-319-31204-0_9">Automating biomedical data science through tree-based pipeline optimization</a><br />
</li>
<li>My personal favorite is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5882490/">this paper</a> where the authors did a really nice job on explaining TPOT to those who are not that familiar with GP</li>
<li>My <a href="https://doi.org/10.1093/bioinformatics/btz470">latest paper</a> on the two new features of TPOT: <code>template</code> and <code>FeatureSetSelector</code></li>
</ul>
<p>While current version of TPOT is already quite user friendly (especially compared to other autoML tools out there), I figured a list of common tips and tricks may be helpful to new users. Please let me know if I miss anything or if you have additional questions you’d like to ask. Here we go!</p>
<div id="where-do-i-start" class="section level4">
<h4>Where do I start?</h4>
<p>TPOT has greatly extensive documentation at <a href="http://epistasislab.github.io/tpot/">http://epistasislab.github.io/tpot/</a>, so I would start there. You will find on that site the instruction for installation and example code for a few classification and regression problems. These examples are as simple as</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> tpot <span class="im">import</span> TPOTClassifier</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a>seed <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb1-8"><a href="#cb1-8"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(iris.data.astype(np.float64),</span>
<span id="cb1-9"><a href="#cb1-9"></a>    iris.target.astype(np.float64), train_size<span class="op">=</span><span class="fl">0.75</span>, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span>seed)</span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a>tpot <span class="op">=</span> TPOTClassifier(generations<span class="op">=</span><span class="dv">5</span>, population_size<span class="op">=</span><span class="dv">50</span>, verbosity<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span>seed)</span>
<span id="cb1-12"><a href="#cb1-12"></a>tpot.fit(X_train, y_train)</span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="bu">print</span>(tpot.score(X_test, y_test))</span>
<span id="cb1-14"><a href="#cb1-14"></a>tpot.export(<span class="st">&#39;tpot_iris_pipeline.py&#39;</span>)</span></code></pre></div>
<p>The last line of this block code exports the <code>tpot_iris_pipeline.py</code> file.
<strong>Caution!</strong> We need to specify the same seed for the <code>random_state</code> argument in this <code>train_test_split()</code> and <code>TPOTClassifier()</code> as the above code chunk did.
This way, the <code>train_test_split</code> function in the exported file <code>tpot_iris_pipeline.py</code> will have the same <code>random_state</code> argument as in the originally split.
Of course, if you’re using a completely independent dataset for validation, this won’t be an issue.</p>
</div>
<div id="what-if-i-find-a-bug-can-i-see-the-source-code" class="section level4">
<h4>What if I find a bug? Can I see the source code?</h4>
<p>Yes, you may report an issue at TPOT’s <a href="https://github.com/EpistasisLab/tpot">GitHub repository</a>.
Also, if you would like to contribute (<em>e.g.</em> fix a bug), you might find the <a href="https://epistasislab.github.io/tpot/contributing/">contributing guidelines</a> helpful.</p>
</div>
<div id="what-pipeline-operators-are-included-in-tpot-and-what-are-the-ranges-for-their-hyperparameters" class="section level4">
<h4>What pipeline operators are included in TPOT? And what are the ranges for their hyperparameters?</h4>
<p>TPOT includes most operators (transformers, feature selectors, classifiers and regressors) from <a href="https://scikit-learn.org/stable/">scikit-learn</a> and XGBoost.
For example, in classification problems, all the operators are included <a href="https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py">here</a> which also has the details on the ranges for corresponding hyperparameters.
See the parent directory for other operators used for, say, regression problems.</p>
</div>
<div id="can-i-customize-the-set-of-operators-for-tpot-to-consider-in-the-pipelines" class="section level4">
<h4>Can I customize the set of operators for TPOT to consider in the pipelines?</h4>
<p>Definitely! You can include only your favorite operators or remove time-consuming operators by modifying the <code>config_dict</code> parameter in the <code>TPOTClassifier</code> or <code>TPOTRegression</code> function. More details <a href="https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters">here</a>.</p>
</div>
<div id="can-i-use-tpot-to-tune-hyperparameters" class="section level4">
<h4>Can I use TPOT to tune hyperparameters?</h4>
<p>Yup!
There are actually multiple ways to do this.
You can reduce the <code>config_dict</code> argument to your own classifier as described above.
For example, to tune a random forest model:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>tpot_config <span class="op">=</span> {</span>
<span id="cb2-2"><a href="#cb2-2"></a>   <span class="st">&#39;sklearn.ensemble.RandomForestClassifier&#39;</span>: {</span>
<span id="cb2-3"><a href="#cb2-3"></a>        <span class="st">&#39;n_estimators&#39;</span>: [<span class="dv">1000</span>],</span>
<span id="cb2-4"><a href="#cb2-4"></a>        <span class="st">&#39;criterion&#39;</span>: [<span class="st">&quot;gini&quot;</span>],</span>
<span id="cb2-5"><a href="#cb2-5"></a>        <span class="st">&#39;max_features&#39;</span>: np.arange(<span class="fl">0.05</span>, <span class="fl">1.01</span>, <span class="fl">0.1</span>),</span>
<span id="cb2-6"><a href="#cb2-6"></a>        <span class="st">&#39;min_samples_split&#39;</span>: <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">21</span>),</span>
<span id="cb2-7"><a href="#cb2-7"></a>        <span class="st">&#39;min_samples_leaf&#39;</span>:  <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">21</span>),</span>
<span id="cb2-8"><a href="#cb2-8"></a>        <span class="st">&#39;bootstrap&#39;</span>: [<span class="va">True</span>, <span class="va">False</span>]</span>
<span id="cb2-9"><a href="#cb2-9"></a>    }</span>
<span id="cb2-10"><a href="#cb2-10"></a>}</span>
<span id="cb2-11"><a href="#cb2-11"></a></span>
<span id="cb2-12"><a href="#cb2-12"></a>tpot <span class="op">=</span> TPOTClassifier(generations<span class="op">=</span><span class="dv">5</span>, population_size<span class="op">=</span><span class="dv">20</span>, verbosity<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-13"><a href="#cb2-13"></a>                      config_dict<span class="op">=</span>tpot_config)</span></code></pre></div>
<p>Also, with a customized <code>config_dict</code>, you can define the parameter grid however you want.</p>
<p>Another (simpler) way to tune your hyperparameters for a classifier or regressor is to use template:
For example, if you want to vary a random forest’s hyperparameters in default <a href="https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py">range</a>, you can specify <code>template = "RandomForestClassifier"</code>.
You can of course add to template other operators such as a feature selector.</p>
</div>
<div id="can-i-include-my-own-classifier-in-tpot" class="section level4">
<h4>Can I include my own classifier in TPOT?</h4>
<p>This is a little bit trickier, but yes, you can, as long as your functions follow the scikit-learn syntax. Please see <a href="https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/preprocessing/data.py#L1047">here</a> and <a href="https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/ensemble/gradient_boosting.py#L1778">here</a> for syntax examples of scikit-learn transformers and classifiers, respectively.</p>
</div>
<div id="why-is-my-regression-cross-validated-score-so-low" class="section level4">
<h4>Why is my regression cross-validated score so low?</h4>
<p>Default for scoring regression in TPOT is negative mean squared error. Therefore, if you want to maximize <span class="math inline">\(R^2\)</span> instead, you’ll have to modify the objective function via the <code>scoring</code> argument. <a href="https://epistasislab.github.io/tpot/using/#scoring-functions">Here</a> is how.
Note that you can also define your own scoring function!</p>
</div>
<div id="is-there-a-place-where-i-can-get-all-the-tpot-default-parameters-e.g.-the-default-scoring-function-mutation-rate-etc." class="section level4">
<h4>Is there a place where I can get all the TPOT default parameters, <em>e.g.</em> the default scoring function, mutation rate, <em>etc.</em>?</h4>
<p>Yes, please see <a href="https://epistasislab.github.io/tpot/api/">TPOT API</a>.</p>
</div>
<div id="how-do-i-know-which-features-were-selected-in-the-exported-pipeline" class="section level4">
<h4>How do I know which features were selected in the exported pipeline?</h4>
<p>If you allow your <code>config_dict</code> parameter to contain <code>Selector</code> or <code>FeatureSetSelector</code>, the exported pipeline may look something like this:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>exported_pipeline <span class="op">=</span> make_pipeline(</span>
<span id="cb3-2"><a href="#cb3-2"></a>    SelectPercentile(score_func<span class="op">=</span>f_classif, percentile<span class="op">=</span><span class="dv">58</span>),</span>
<span id="cb3-3"><a href="#cb3-3"></a>    OneHotEncoder(minimum_fraction<span class="op">=</span><span class="fl">0.2</span>, sparse<span class="op">=</span><span class="va">False</span>, threshold<span class="op">=</span><span class="dv">10</span>),</span>
<span id="cb3-4"><a href="#cb3-4"></a>    GradientBoostingClassifier(learning_rate<span class="op">=</span><span class="fl">0.01</span>, max_depth<span class="op">=</span><span class="dv">9</span>, max_features<span class="op">=</span><span class="fl">0.1</span>, min_samples_leaf<span class="op">=</span><span class="dv">2</span>, min_samples_split<span class="op">=</span><span class="dv">19</span>, n_estimators<span class="op">=</span><span class="dv">100</span>, subsample<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a>)</span></code></pre></div>
<p>Then, to get the selected features, you can utilize the <code>get_support()</code> function:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>exported_pipeline.steps[<span class="dv">0</span>][<span class="dv">1</span>].get_support()</span></code></pre></div>
</div>
<div id="how-do-i-know-which-features-are-important-in-my-optimal-pipeline" class="section level4">
<h4>How do I know which features are important in my optimal pipeline?</h4>
<p>I would use <a href="https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html">Eli</a> accompanied with the exported pipeline to compute the permutation importance score.
Here are a few lines of example code:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">from</span> eli5.sklearn <span class="im">import</span> PermutationImportance</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a>model <span class="op">=</span> exported_pipeline.fit(training_features, training_target)</span>
<span id="cb5-4"><a href="#cb5-4"></a>perm <span class="op">=</span> PermutationImportance(model, n_iter<span class="op">=</span><span class="dv">100</span>).fit(training_features, training_target)</span>
<span id="cb5-5"><a href="#cb5-5"></a>feat_imp <span class="op">=</span> perm.feature_importances_</span></code></pre></div>
<p>Alternatively, you may want to try the <code>permutation_importance()</code> function in the <a href="https://scikit-learn.org/dev/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance">under-development version</a> of scikit-learn.</p>
</div>
<div id="why-cant-i-reproduce-my-results" class="section level4">
<h4>Why can’t I reproduce my results?</h4>
<p>Oooh… This is a tricky one. Let’s walk through a few steps:</p>
<ul>
<li>Did you set <code>random_state</code> in your <code>train_test_split</code>? Or better yet, <code>np.random.seed()</code> at the beginning of your notebook?</li>
<li>Did you set <code>random_state</code> in your <code>TPOTClassifier</code>?</li>
<li>Did you use the <code>set()</code> function somewhere in the notebook? Sets are unordered, and so the elements are randomly permuted.</li>
</ul>
<p>Please report an issue if you still can’t reproduce the results after these checks.</p>
</div>
<div id="its-slow.-what-can-i-do" class="section level4">
<h4>It’s slow. What can I do?</h4>
<p>Evolutionary algorithms are generally not fast.
However, there are a few ways you can try to speed up the training of TPOT:</p>
<ul>
<li>use the template option to contraint the search space of TPOT, <em>e.g.</em>, try <code>template = "FeatureSetSelector-Selector-Transformer-Classifier"</code>{{%sn%}}Note that you won’t get very complicated pipelines, for better or worse. {{%/sn%}}</li>
<li>reduce your dataset dimension, <em>i.e.</em> keep only relevant features/attributes/variables</li>
<li>set <code>n_jobs = -1</code> in your <code>TPOTClassifier()</code> function to use as many cores as available on your machine</li>
<li>TPOT supports <a href="https://docs.dask.org/en/latest/">Dask</a> for parallel training. Follow this <a href="https://examples.dask.org/machine-learning/tpot.html">example</a> to see how they integrate.</li>
</ul>
</div>
<div id="can-i-obtain-the-probability-predictions-from-the-exported-pipeline" class="section level4">
<h4>Can I obtain the probability predictions from the exported pipeline?</h4>
<p>While this is slightly outside of the scope of TPOT, I’ll go ahead and answer it here.
This is straightforward if your final model supports probability prediction.
Otherwise, we can use <a href="https://scikit-learn.org/stable/modules/calibration.html#calibration">probability calibration</a>, which can easily be done with scikit-learn’s <code>CalibratedClassifierCV</code>, to obtain the probabilities on the test set.
Here are a few lines of example code:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(exported_pipeline, <span class="st">&#39;predict_proba&#39;</span>):</span>
<span id="cb6-2"><a href="#cb6-2"></a>  pred_prob <span class="op">=</span> exported_pipeline.predict_proba(testing_features)</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="cf">else</span>:</span>
<span id="cb6-4"><a href="#cb6-4"></a>  clf <span class="op">=</span> CalibratedClassifierCV(exported_pipeline) </span>
<span id="cb6-5"><a href="#cb6-5"></a>  clf.fit(training_features, training_target)</span>
<span id="cb6-6"><a href="#cb6-6"></a>  pred_prob <span class="op">=</span> clf.predict_proba(testing_features)</span></code></pre></div>
</div>
<div id="any-other-resources" class="section level4">
<h4>Any other resources?</h4>
<p>I have given a few TPOT introduction talks.
You may find the <a href="https://slides.com/trang1618/tpot-cic#/0/1">overview slides</a> helpful.
Cheers!</p>
<p><img src="tpot-slide.png" /></p>
</div>
